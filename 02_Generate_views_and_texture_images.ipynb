{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad25227",
   "metadata": {},
   "source": [
    "## Load the metadata\n",
    "\n",
    "The metadata pre-processed in notebook 1 contains the id's of the 3D-models that will be downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "035e197e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 971 3D-models in the dataset.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# reload the cleaned-up data\n",
    "metadata_df = pd.read_csv(os.path.join('data', 'metadata.csv'), index_col=False)\n",
    "print('There are {} 3D-models in the dataset.'.format(metadata_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48829ce9",
   "metadata": {},
   "source": [
    "## Downloading 3D models\n",
    "I obtained 3D-models from the [Sketchfab website](https://sketchfab.com/3d-models/categories/cultural-heritage-history?date=week&features=downloadable&licenses=322a749bcfa841b29dff1e8a1bb74b0b&licenses=b9ddc40b93e34cdca1fc152f39b9f375&licenses=72360ff1740d419791934298b8b6d270&licenses=bbfe3f7dbcdd4122b966b85b9786a989&licenses=2628dbe5140a4e9592126c8df566c0b7&licenses=34b725081a6a4184957efaec2cb84ed3&licenses=7c23a1ba438d4306920229c12afcb5f9&licenses=783b685da9bf457d81e829fa283f3567&licenses=5b54cf13b1a4422ca439696eb152070d&sort_by=-likeCount). Sketchfab provides an [API](https://docs.sketchfab.com/data-api/v3/index.html#!/models/get_v3_models_uid_download) for querying and downloading 3D-models. I contacted Sketchfab to check if it would be alright to download a large number of 3D-models. They have been very supportive to my idea and I managed to use the API to download all the metadata and a few 3D-models. \n",
    "The API requires a Sketchfab account and a key. The key is stored in the file `client_credentials.json` (not included in the submission). Please note that:\n",
    "* 3D-models are large files. The project should require an estimated 20 GB of data, for the downloaded files alone. I setup a virtual storage with 50 GB to hold the data for this project. Github recommends keeping projects under 5 GB. Therefore I will not be able to upload the 3D-models as such to the capstone repository, but only the pre-processed files. \n",
    "* the Sketchfab API has various download rate restrictions (per day, per hour, per minute), so that the download process has to be extended over a period of several days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "821c1101",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /mnt/Disk2/downloaded_models/a049baa005da476bb7ffe1df5e6b4f78.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/836630c9eb6344878f5b3e72aaa37f8b.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/77d11d8ec01e424e829b9914c388b0a1.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/c9c8bce4d27a4fef88622682bf2741b1.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/991d3fd5aab643609819ade2a1df1eb2.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/89fdc50c378c4b8ea4348f673318a179.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/f5a96c73c0eb49a1b787ae258eb02891.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/ab333c7e6d0f41cbafbbc36b634488a5.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/fbe2776af5f44d83bb32e6633d151178.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/72aba513c29241a3a0843a14dba596a5.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/7bbe7a566f074896b9cffea1e353275a.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/036abe36af224e588d3f573f44c24ab3.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/3af9453ca5694a3aae66a05f27d94881.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/c7c762e635ab4403b7363860665009ac.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/07ad36c0658e4eafa91f0137e49fad58.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/5cd861450b544fccb7ac3ecf525afadd.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/c5374274b68e4d338880f6d7ac79e062.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/d9fa455509ea4bc4b9acde9b7d20df14.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/ec08a45c7aa74ed4945827c47912b0c4.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/bc99f1acf7764a91ba6800864c6124c2.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/5e39b7649dc4456e9b911059a3c4c0ba.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/ccdf0d3aabe6468493c5b87e29a10bef.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/903905eb713245efbf1769456822fa5c.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/bfae6fa9a7f64b9eb16f57c05553c612.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/2cf72e290a2e4849841df9d29905911e.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/4dd0239e30844be5a9dffdcde2bde844.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/74b5c985f7a747fe9863f3b5f977a056.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/1661929dfcbf4a829b8f81512a767f92.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/663fd55d530c4f1abeefb92b7be45502.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/09b62c5be969464a99a58eb5160e3122.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/5c5c3b9ae7874b709c10ac57dad33195.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/fa6cfc147c1c44ccb014e5c29cde27c9.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/87e986b6a3784421a82b86f9a0977183.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/826eeb384b644d278e78c802ac59fbc3.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/ad1c9d8e81b04e13bf4220675337f04f.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/7d3689df8ca74f97a0bd70c3bc8f7619.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/c65c0fae7d0e4bbca21839fb00dabb0f.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/e2aba8bd92fb4247913614058408e4e8.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/7c2fde254a1741a6b2b99d8d2adcfd1c.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/40c79ef7dc9c466da86c44851103dee5.zip already exists\n",
      "File /mnt/Disk2/downloaded_models/e2ce3cedb2d144508564661251dff30c.zip already exists\n",
      "Downloading model 0a17eb9617f04a7bab8fc04c57f2391f (42/922)\n",
      "Download failed for 3D-model: 0a17eb9617f04a7bab8fc04c57f2391f\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "# load credentials stored in local file\n",
    "with open('client_credentials.json') as file:\n",
    "    client_credentials = json.load(file)\n",
    "API_TOKEN = client_credentials['api_token'] # Token from Sketchfab account\n",
    "\n",
    "DOWNLOAD_DIR = '/mnt/Disk2/downloaded_models' # Folder to store the downloaded files\n",
    "DOWNLOAD_DELAY = 20 # seconds, 200 / hour API rate limit\n",
    "DOWNLOAD_ENDPOINT = 'https://api.sketchfab.com/v3/models/{}/download' # API endpoint\n",
    "\n",
    "# HTTP request headers\n",
    "headers = {'Authorization': 'Token {}'.format(API_TOKEN)} # Authorization method used\n",
    "headers.update({'Content-Type': 'application/json'}) # http content type of the request\n",
    "\n",
    "def download_data(uid):\n",
    "    \"\"\"\n",
    "    Downloads a 3D-model from Sketchfab, saves it to a zipped file.\n",
    "    @param: uid unique id of the model\n",
    "    @raises: RequestException, e.g. API limit exceeded\n",
    "    \"\"\"\n",
    "    # get the download url of the model\n",
    "    # this url is only valid for a few minutes\n",
    "    r = requests.get(\n",
    "        DOWNLOAD_ENDPOINT.format(model_uid),\n",
    "        headers = headers\n",
    "    )\n",
    "    r.raise_for_status() # throw exception if request status is not OK\n",
    "    model_link = json.loads(r.text)\n",
    "    url = model_link['gltf']['url'] # download url of the model\n",
    "\n",
    "    # download the 3D-model, save it to file\n",
    "    r = requests.get(url)\n",
    "    with open(filename,'wb') as output_file:\n",
    "        output_file.write(r.content)\n",
    "            \n",
    "\n",
    "try:\n",
    "    # loop through the metadata table and download each 3D-model\n",
    "    total = len(metadata_df)\n",
    "    counter = 1\n",
    "    for model_uid in metadata_df['id']:\n",
    "        filename = os.path.join(DOWNLOAD_DIR, '{}.zip'.format(model_uid)) # file to save the 3D-model\n",
    "        if os.path.isfile(filename): # check that the file hasn't been downloaded already\n",
    "            print('File {} already exists'.format(filename))\n",
    "        else:\n",
    "            print('Downloading model {} ({}/{})'.format(model_uid, counter, total))\n",
    "            download_data(model_uid) # download and save the 3D-model\n",
    "            time.sleep(DOWNLOAD_DELAY) # don't hit the API too hard\n",
    "        counter += 1\n",
    "\n",
    "# Downloads may fail, typically when API limit exceeded\n",
    "except requests.exceptions.RequestException:\n",
    "    print('Download failed for 3D-model: {}'.format(model_uid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d49fce",
   "metadata": {},
   "source": [
    "## Pre-processing 3D-models\n",
    "\n",
    "The files containing the 3D-models have to be unzipped to a temporary folder. 3D-Models provided by the API are in Graphics Language Transmission Format (glTF). I implemented a reader for this kind of files using the open source [pygltflib library](https://pypi.org/project/pygltflib/). This library is __not included in the course environment, but can be installed using pip__. \n",
    "\n",
    "<img src=\"fig/point_cloud_texture.png\">\n",
    "\n",
    "_Figure 3: A Greek jug (A), its 3D point cloud (B), its texture image (C)._\n",
    "\n",
    "After extracting the texture and the point cloud data from each 3D-model,\n",
    "* I downsampled texture data to the size expected by the data extraction model, i.e. 299x299 pixels RGB, and I applied standardization to the texture images.\n",
    "* I downsampled point cloud data so that the number of vertices is the number computed in the EDA in notebook 1, i.e. 37838 vertices.\n",
    "\n",
    "I saved the downsampled data of each 3D-model in an .npz file containing \"point cloud\" data, and a second .npz file for texture images (fig. 3). The classifiers I implemented use either point cloud or texture data for input, so organizing these data separately at this stage will be beneficial later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f15d4",
   "metadata": {},
   "source": [
    "### Implement a function to decode GLTF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed4c9ee9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pygltflib import GLTF2, BufferFormat\n",
    "import struct\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # the gltf loader raises some unecessary warning\n",
    "\n",
    "DOWNLOAD_DIR = '/mnt/Disk2/downloaded_models' # Folder where the downloaded files are stored\n",
    "TEMP_DIR = '/mnt/Disk2/temp' # Folder to temporarily extract the zipped files\n",
    "\n",
    "def extract_gltf(model_uid):\n",
    "    \"\"\"\n",
    "    Extracts the point cloud data from a gltf file as a numpy array.\n",
    "    The returned array contains the 3D coordinates of all points in the point cloud.\n",
    "    @param model_uid, string, id of a model\n",
    "    @returns a numpy array of 3D coordinates\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # create a temporary folder to extract this file\n",
    "        unzip_dir = os.path.join(TEMP_DIR, model_uid)\n",
    "        os.mkdir(unzip_dir)\n",
    "\n",
    "        # unzip the downloaded 3D-model\n",
    "        filename_zipped = os.path.join(DOWNLOAD_DIR, '{}.zip'.format(model_uid)) # zip file containing the the 3D-model\n",
    "        with zipfile.ZipFile(filename_zipped, 'r') as zip_file:\n",
    "            zip_file.extractall(unzip_dir)\n",
    "\n",
    "        # After unzipping the file, open its \"scene\" data\n",
    "        filename = os.path.join(unzip_dir, 'scene.gltf') # the \"scene\" file is always called scene.gltf\n",
    "        gltf = GLTF2().load(filename)\n",
    "        # load the binary data contained in the \"scene.bin\" into the gltf object\n",
    "        gltf.convert_buffers(BufferFormat.DATAURI)\n",
    "\n",
    "        # get the vertices for each primitive in each mesh\n",
    "        vertices = []\n",
    "        for mesh in gltf.meshes:\n",
    "            for primitive in mesh.primitives:\n",
    "\n",
    "                # get the binary data for this mesh primitive from the buffer\n",
    "                accessor = gltf.accessors[primitive.attributes.POSITION]\n",
    "                bufferView = gltf.bufferViews[accessor.bufferView]\n",
    "                buffer = gltf.buffers[bufferView.buffer]\n",
    "                data = gltf.decode_data_uri(buffer.uri)\n",
    "\n",
    "                # pull each vertex from the binary buffer and convert it into a tuple of python floats\n",
    "                for i in range(accessor.count):\n",
    "                    index = bufferView.byteOffset + accessor.byteOffset + i*12  # the location in the buffer of this vertex\n",
    "                    d = data[index:index+12]  # the vertex data\n",
    "                    v = struct.unpack(\"<fff\", d)   # convert from base64 to three floats\n",
    "                    vertices.append(v)\n",
    "\n",
    "        # convert the vertices to a numpy array\n",
    "        vertices_np = np.array(vertices)\n",
    "        return vertices_np\n",
    "    \n",
    "    finally:\n",
    "        # whatever happens, delete the temporary folder\n",
    "        os.remove(os.path.join(unzip_dir, 'scene.gltf'))\n",
    "        os.remove(os.path.join(unzip_dir, 'scene.bin'))\n",
    "        os.rmdir(unzip_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca578dd6",
   "metadata": {},
   "source": [
    "### Implement a function to subsample point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdadf859",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 3D-models will be downsampled to 37838 vertices.\n"
     ]
    }
   ],
   "source": [
    "MAX_VERTICES = metadata_df['vertexCount'].min() # the lowest vertex count of any model in the dataset\n",
    "print('The 3D-models will be downsampled to {} vertices.'.format(MAX_VERTICES))\n",
    "\n",
    "def subsample_model(vertices_np):\n",
    "    \"\"\"\n",
    "    Reduces the number of vertices in the 3D-model.\n",
    "    @param vertices_np np array as returned by extract_gltf()\n",
    "    @returns np array with MAX_VERTICES vertices\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(vertices_np).sample(n=MAX_VERTICES, random_state=0).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc7eb2",
   "metadata": {},
   "source": [
    "### Pre-process the downloaded 3D-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4ee573d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# get a list of all 3D-models that have been downloaded\n",
    "downloaded_uid = [model_uid for model_uid in metadata_df['id'] if os.path.isfile(os.path.join(DOWNLOAD_DIR, '{}.zip'.format(model_uid)))]\n",
    "\n",
    "vertices = []\n",
    "for model_uid in downloaded_uid:\n",
    "    # read the point cloud and subsample the vertices\n",
    "    vertices_np = extract_gltf(model_uid)\n",
    "    vertices_np = subsample_model(vertices_np)\n",
    "    vertices.append(vertices_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5aff54",
   "metadata": {},
   "source": [
    "The pre-processed 3D-data can now be saved to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed562a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# save all extracted vertices to compressed file\n",
    "np.savez_compressed(os.path.join('data', 'point_clouds.npz'),\n",
    "         model_uid = downloaded_uid,\n",
    "         vertices = vertices\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
