{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "146cd6f4",
   "metadata": {},
   "source": [
    "# Feature extraction and post-extraction EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395da217",
   "metadata": {},
   "source": [
    "## Choice of model\n",
    "The [Inception v3](https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1) and [MobileNet v2](https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/2) are both up to the task, their TF Hub implementations have been trained on the ILSVRC-2012-CLS \"ImageNet\" data set, and have the [same signature for feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector).\n",
    "\n",
    "The MobileNet v2 is optimized for mobile applications. Since I'm not building a mobile application, I chose the Inception v3 model. This model has more parameters, expects slightly larger input images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e57811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.12.0, TF hub version: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print('Tensorflow version: {}, TF hub version: {}'.format(tf.__version__, hub.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1129e78",
   "metadata": {},
   "source": [
    "## Create a network\n",
    "Create a network by loading the chosen pre-trained model from TF Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b3ffdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "img_graph = tf.Graph()\n",
    "\n",
    "with img_graph.as_default():\n",
    "    # pretrained network Inception v3\n",
    "    module_url = 'https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1'\n",
    "    feature_extractor = hub.Module(module_url)\n",
    "    \n",
    "    # expected size of input images\n",
    "    height, width = hub.get_expected_image_size(feature_extractor)\n",
    "    \n",
    "    # placeholder for input\n",
    "    input_imgs = tf.placeholder(dtype=tf.float32, shape=[None, height, width, 3])\n",
    "    \n",
    "    # node that represents extracted high-level features\n",
    "    imgs_features = feature_extractor(input_imgs)\n",
    "    \n",
    "    # initializers required by TensorFlow Hub\n",
    "    init_op = tf.group(\n",
    "        [tf.global_variables_initializer(), tf.tables_initializer()]\n",
    "    )\n",
    "\n",
    "img_graph.finalize() # make graph read-only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5503c3",
   "metadata": {},
   "source": [
    "### Implement a function to load data from point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1af86f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "TF_SIZE = 299\n",
    "\n",
    "def convert_images(data):\n",
    "    \"\"\"\n",
    "    Standardize and convert the image data\n",
    "    @param np_image_array \n",
    "    \"\"\"\n",
    "    X = []\n",
    "    for idx in range(len(data)):\n",
    "        flat_img = data[idx, :] # get some image\n",
    "        flat_img = flat_img/255  # standardize values to 0-1\n",
    "        img = flat_img.reshape(299,299, 3) # sqrt 784\n",
    "        X.append(img)\n",
    "    return (img for img in X)\n",
    "\n",
    "def load_point_cloud(dataset_name):\n",
    "    \"\"\"\n",
    "    Load point cloud data\n",
    "    @param dataset_name string 'train', 'test' or 'validate'\n",
    "    @returns numpy.ndarray metadata, numpy.ndarray 299*299 3-channel flattened Images elevation, plan, section\n",
    "    \"\"\"\n",
    "    with np.load(os.path.join('data', dataset_name, 'point_clouds.npz'), allow_pickle=True) as data:\n",
    "        data_dict = dict(data.items())\n",
    "\n",
    "    metadata = data_dict['metadata']\n",
    "    elevation = convert_images(data_dict['elevation'])\n",
    "    plan = convert_images(data_dict['plan'])\n",
    "    section = convert_images(data_dict['section'])\n",
    "    return metadata, elevation, plan, section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc1bc42",
   "metadata": {},
   "source": [
    "### Implement a function to load textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d388e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_textures(dataset_name):\n",
    "    \"\"\"\n",
    "    Load texture data\n",
    "    @param dataset_name string 'train', 'test' or 'validate'\n",
    "    @returns numpy.ndarray metadata, numpy.ndarray 299*299 3-channel flattened Images\n",
    "    \"\"\"\n",
    "    with np.load(os.path.join('data', dataset_name, 'textures.npz'), allow_pickle=True) as data:\n",
    "        data_dict = dict(data.items())\n",
    "\n",
    "    metadata = data_dict['metadata']\n",
    "    textures = data_dict['texture']\n",
    "    return metadata, textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "198f871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation dataset\n",
    "_, textures = load_textures('validate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e93c5d",
   "metadata": {},
   "source": [
    "### Extract features\n",
    "Extract and save the features from point cloud data for each of the train, validate and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ea7cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROD_DIR = 'production' # interim production files\n",
    "\n",
    "sess = tf.Session(graph=img_graph)\n",
    "sess.run(init_op)\n",
    "\n",
    "def extract_features(images):\n",
    "    \"\"\"\n",
    "    Extracts high-level features from images.\n",
    "    @param images image generator\n",
    "    @return list\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for img in images:\n",
    "        features.append(\n",
    "            sess.run(\n",
    "                imgs_features,\n",
    "                feed_dict = {\n",
    "                    input_imgs: [img]\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return features\n",
    "\n",
    "def save_dataset_features(dataset_name):\n",
    "    \"\"\"\n",
    "    Saves features extracted from a dataset as npz file\n",
    "    @param dataset_name 'train', 'validate' or 'test'\n",
    "    \"\"\"\n",
    "    metadata, elevation, plan, section = load_point_cloud(dataset_name)\n",
    "    elevation_features = extract_features(elevation)\n",
    "    plan_features = extract_features(plan)\n",
    "    section_features = extract_features(section)\n",
    "\n",
    "    np.savez_compressed(os.path.join(PROD_DIR, 'features_{}.npz'.format(dataset_name)), \n",
    "            metadata = metadata,\n",
    "            elevation_features = elevation_features,\n",
    "            plan_features = plan_features,\n",
    "            section_features = section_features    \n",
    "            )\n",
    "\n",
    "save_dataset_features('train')\n",
    "save_dataset_features('validate')\n",
    "save_dataset_features('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f75b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
